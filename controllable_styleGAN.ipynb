{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HjsfDFjHJLz9"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow_hub'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfds\u001b[39;00m \u001b[38;5;66;03m# Uncomment to import tensorflow_datasets\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_hub\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mthub\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conv2D, Dense, Reshape, Flatten, Dropout, LeakyReLU, Conv2DTranspose, BatchNormalization\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_hub'"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds # Uncomment to import tensorflow_datasets\n",
        "import tensorflow_hub as thub\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, Dense, Reshape, Flatten, Dropout, LeakyReLU, Conv2DTranspose, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8d6V_if0JLz_"
      },
      "outputs": [],
      "source": [
        "# Load CelebA data\n",
        "# dataset, info = tfds.load('celeb_a', with_info = True, as_supervised = True)\n",
        "# train_ds = dataset['train']\n",
        "# test_ds = dataset['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FonP1m5fJLz_"
      },
      "outputs": [],
      "source": [
        "# Generator model\n",
        "def Generator(z_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(7*7*256, input_dim=z_dim))\n",
        "    model.add(Reshape((7, 7, 256)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "    model.add(Conv2DTranspose(256, kernel_size=4, strides=2, padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "    model.add(Conv2DTranspose(128, kernel_size=4, strides=2, padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "    model.add(Conv2DTranspose(64, kernel_size=4, strides=2, padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "    model.add(Conv2DTranspose(1, kernel_size=4, strides=1, padding='same', activation='tanh'))\n",
        "    return model\n",
        "\n",
        "# Discriminator model\n",
        "def Classifier(img_shape, n_classes):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(64, kernel_size=4, strides=2, padding='same', input_shape=img_shape))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Conv2D(128, kernel_size=4, strides=2, padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Conv2D(128, kernel_size=4, strides=2, padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(n_classes, activation='softmax'))\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2ecs4yTSJL0A"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'thub' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[11], line 17\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Uncomment to manually train a classifier\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Classifier = Classifier((28, 28, 1), n_classes)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Classifier.compile(optimizer = Adam(learning_rate=0.002), loss = 'categorical_crossentropy', matrics = ['acccuracy'])\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Load a pretrained classifier on celebA\u001b[39;00m\n\u001b[0;32m     15\u001b[0m model_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://tfhub.dev/google/imagenet/mobilenet_v2_100_224/classification/5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     16\u001b[0m classifier \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[1;32m---> 17\u001b[0m     \u001b[43mthub\u001b[49m\u001b[38;5;241m.\u001b[39mKerasLayer(model_url, input_shape\u001b[38;5;241m=\u001b[39mimg_shape)\n\u001b[0;32m     18\u001b[0m ])\n",
            "\u001b[1;31mNameError\u001b[0m: name 'thub' is not defined"
          ]
        }
      ],
      "source": [
        "# Define GAN components\n",
        "n_classes = 40 # 40 classes in celebA\n",
        "z_dim = 128\n",
        "batch_size = 256\n",
        "img_shape = (56, 56, 3)\n",
        "\n",
        "# Initialize models\n",
        "generator = Generator(z_dim)\n",
        "\n",
        "# Uncomment to manually train a classifier\n",
        "# Classifier = Classifier((28, 28, 1), n_classes)\n",
        "# Classifier.compile(optimizer = Adam(learning_rate=0.002), loss = 'categorical_crossentropy', matrics = ['acccuracy'])\n",
        "\n",
        "# Load a pretrained classifier on celebA\n",
        "model_url = \"https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/classification/5\"\n",
        "classifier = tf.keras.Sequential([\n",
        "    thub.KerasLayer(model_url, input_shape=img_shape)\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoKDTfXzJL0A"
      },
      "outputs": [],
      "source": [
        "generator.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_T5p_GTGa7kv"
      },
      "outputs": [],
      "source": [
        "def gradient_penalty(discriminator, real_imgs, fake_imgs, epsilon, batch_size):\n",
        "    interpolated_imgs = epsilon * tf.cast(real_imgs, tf.float32) + ((1 - epsilon) * tf.cast(fake_imgs, tf.float32))\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(interpolated_imgs)\n",
        "        pred = discriminator(interpolated_imgs, training = True)\n",
        "\n",
        "    gradients = tape.gradient(pred, [interpolated_imgs])[0]\n",
        "    grad_norms = tf.sqrt(tf.reduce_sum(tf.square(gradients), axis=[1, 2, 3]))\n",
        "    gradient_penalty = tf.reduce_mean(tf.square(grad_norms - 1))\n",
        "\n",
        "    return gradient_penalty\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4IQTo9JJL0A"
      },
      "outputs": [],
      "source": [
        "# Function to plot generated images\n",
        "def plot_generated_images(generator, epoch, z_dim, examples=16, dim=(4, 4), figsize=(10, 10)):\n",
        "    noise = np.random.normal(0, 1, size=[examples, z_dim])\n",
        "    generated_images = generator.predict(noise)\n",
        "    generated_images = generated_images.reshape(examples, 28, 28)\n",
        "\n",
        "    plt.figure(figsize=figsize)\n",
        "    for i in range(examples):\n",
        "        plt.subplot(dim[0], dim[1], i + 1)\n",
        "        plt.imshow(generated_images[i], interpolation='nearest', cmap='gray')\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'gan_output_sample_{epoch}.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEe89n78u8f5"
      },
      "outputs": [],
      "source": [
        "gen_optimizer = Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.9)\n",
        "critic_optimizer = Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_losses(critic_losses, gen_losses):\n",
        "    # Plot losses\n",
        "    plt.figure(figsize=(7, 3))\n",
        "    plt.plot(critic_losses, label='Critic Loss')\n",
        "    plt.plot(gen_losses, label='Generator Loss')\n",
        "    plt.xlabel('Batch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsulnIYFJL0A"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_critic(discriminator, critic_optimizer, real_imgs, fake_imgs):\n",
        "    # Train discriminator\n",
        "    with tf.GradientTape() as tape:\n",
        "        critic_loss_real = discriminator(real_imgs)\n",
        "        critic_loss_fake = discriminator(fake_imgs)\n",
        "        gp = gradient_penalty(discriminator, real_imgs, fake_imgs, epsilon, batch_size)\n",
        "        critic_loss = tf.reduce_mean(critic_loss_fake) - tf.reduce_mean(critic_loss_real) + (lambda_gp * gp)\n",
        "    gradients = tape.gradient(critic_loss, discriminator.trainable_variables)\n",
        "    critic_optimizer.apply_gradients(zip(gradients, discriminator.trainable_variables))\n",
        "    return critic_loss\n",
        "\n",
        "@tf.function\n",
        "def train_generator(generator, discriminator, gen_optimizer):\n",
        "    z = np.random.randn(batch_size, z_dim)\n",
        "    valid_y = np.ones((batch_size, 1))\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        critic_pred = discriminator(generator(z), training = True)\n",
        "        g_loss = -tf.reduce_mean(critic_pred)\n",
        "    gradients = tape.gradient(g_loss, generator.trainable_variables)\n",
        "    gen_optimizer.apply_gradients(zip(gradients, generator.trainable_variables))\n",
        "    return g_loss\n",
        "\n",
        "# Training function\n",
        "def train_wgan(generator, discriminator, epochs=100, batch_size=64, n_critic_train = 3):\n",
        "    critic_losses, gen_losses = [], []\n",
        "\n",
        "    # Calculate number of batches per epoch\n",
        "    num_batches = X_train.shape[0] // batch_size\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        np.random.shuffle(X_train)\n",
        "        print(f\"Epoch {epoch}\")\n",
        "        n_critic_count = 1\n",
        "        for batch in range(num_batches):\n",
        "            real_imgs = X_train[batch * batch_size: (batch * batch_size) + batch_size]\n",
        "            z = np.random.randn(batch_size, z_dim)\n",
        "            fake_imgs = generator.predict(z)\n",
        "\n",
        "            # Train critic\n",
        "            critic_loss = train_critic(discriminator, critic_optimizer, real_imgs, fake_imgs)\n",
        "            n_critic_count += 1\n",
        "\n",
        "            if n_critic_count > n_critic_train:\n",
        "                # Train generator\n",
        "                g_loss = train_generator(generator, discriminator, gen_optimizer)\n",
        "                n_critic_count = 1\n",
        "                # Save gen losses\n",
        "                gen_losses.append(g_loss)\n",
        "\n",
        "            # Save critic losses\n",
        "            critic_losses.append(critic_loss)\n",
        "\n",
        "        # Print progress\n",
        "        # os.system('cls' if os.name == 'nt' else 'clear')\n",
        "        print(f\"D Loss: {np.mean(critic_losses)}, G Loss: {np.mean(gen_losses)}\")\n",
        "        plot_generated_images(generator, epoch, z_dim)\n",
        "        show_losses(critic_losses, gen_losses)\n",
        "\n",
        "    return critic_losses, gen_losses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3pfJSeIs1-X"
      },
      "outputs": [],
      "source": [
        "# Train DCGAN\n",
        "D_losses, g_losses = train_wgan(generator, discriminator, epochs = 10, batch_size = batch_size, n_critic_train=3)\n",
        "\n",
        "# Save the generator model\n",
        "generator.save('simple_gan_generator.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpuekLAVJL0A"
      },
      "outputs": [],
      "source": [
        "\n",
        "def generate_images(generator, z_points):\n",
        "    \"\"\"Generate images from z-space points using the generator.\"\"\"\n",
        "    images = generator.predict(z_points)\n",
        "    return images\n",
        "\n",
        "# Generate random points in z-space\n",
        "z_dim = 100  # Dimension of the z-space (latent space)\n",
        "num_samples = 16  # Number of images to generate\n",
        "z_points = np.random.randn(num_samples, z_dim)\n",
        "\n",
        "# Generate images from these points\n",
        "generated_images = generate_images(generator, z_points)\n",
        "\n",
        "# Plot the generated images\n",
        "fig, axes = plt.subplots(4, 4, figsize=(10, 10))\n",
        "for i, ax in enumerate(axes.flatten()):\n",
        "    ax.imshow(generated_images[i].reshape(28, 28), cmap='gray')\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHgqxQZhJL0A"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
