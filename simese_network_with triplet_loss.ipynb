{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Siamese Network with Triplet Loss in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Understanding the Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from pca_plotter import PCAPlotter\n",
    "\n",
    "print('TensorFlow version:', tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Approach\n",
    "\n",
    "This appraoch is taken from the popular [FaceNet](https://arxiv.org/abs/1503.03832) paper.\n",
    "\n",
    "We have a CNN model called `EmbeddingModel`:\n",
    "\n",
    "![CNN](assets/CNN.png)\n",
    "\n",
    "We use three images for each training example:\n",
    "1. `person1_image1.jpg` (Anchor Example, represented below in green)\n",
    "2. `person1_image2.jpg` (Positive Example, in blue)\n",
    "3. `person2_image1.jpg` (Negative Example, in red).\n",
    "\n",
    "![Embeddings](assets/embeddings.png)\n",
    "\n",
    "\n",
    "## Siamese Network\n",
    "\n",
    "All the three images of an example pass through the model, and we get the three Embeddings: One for the Anchor Example, one for the Positive Example, and one for the Negative Example.\n",
    "\n",
    "![Siamese Network](assets/siamese.png)\n",
    "\n",
    "The three instances of the `EmbeddingModel` shown above are not different instances. It's the same, shared model instance - i.e. the parameters are shared, and are updated for all the three paths simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.reshape(x_train, (x_train.shape[0], 784)) /255\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], 784)) /255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Plotting Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  plot_triplet(triplet):\n",
    "    plt.figure(figsize = (6, 2))\n",
    "    for i in range(0,3):\n",
    "        plt.subplot(1, 3, i+ 1)\n",
    "        plt.imshow(np.reshape(triplet[i], (28 , 28)), cmap = 'binary')\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_triplet([x_train[0], x_train[1], x_train[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: A Batch of Triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch(batch_size):\n",
    "    anchors = np.zeros((batch_size, 784))\n",
    "    positives = np.zeros((batch_size, 784))\n",
    "    negatives = np.zeros((batch_size, 784))\n",
    "    for i in range(0, batch_size):\n",
    "        index = random.randint(0, x_train.shape[0] - 1)\n",
    "        anc = x_train[index]\n",
    "        y = y_train[index]\n",
    "\n",
    "        indices_pos = np.squeeze(np.where(y_train == y))\n",
    "        indices_neg = np.squeeze(np.where(y_train != y))\n",
    "\n",
    "        pos = x_train[indices_pos[random.randint(0, len(indices_pos) - 1)]]\n",
    "        neg = x_train[indices_neg[random.randint(0, len(indices_neg) - 1)]]\n",
    "\n",
    "        anchors[i] = anc\n",
    "        positives[i] = pos\n",
    "        negatives[i] = neg\n",
    "    return [anchors, positives, negatives]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet = create_batch(1)\n",
    "plot_triplet(triplet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5: Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 64\n",
    "\n",
    "embedding_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation = 'relu', input_shape = (784, )),\n",
    "    tf.keras.layers.Dense(emb_dim, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "embedding_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = x_train[0]\n",
    "example_embedding = embedding_model.predict(np.expand_dims(example, axis = 0))\n",
    "print(example_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6: Siamese Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_anc = tf.keras.layers.Input(shape = (784, ))\n",
    "in_pos = tf.keras.layers.Input(shape = (784, ))\n",
    "in_neg = tf.keras.layers.Input(shape = (784, ))\n",
    "\n",
    "em_anc = embedding_model(in_anc)\n",
    "em_pos = embedding_model(in_anc)\n",
    "em_neg = embedding_model(in_anc)\n",
    "\n",
    "out = tf.keras.layers.concatenate([em_anc, em_pos, em_neg], axis = 1)\n",
    "\n",
    "net = tf.keras.models.Model([in_anc, in_pos, in_neg], out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 7: Triplet Loss\n",
    "\n",
    "A loss function that tries to pull the Embeddings of Anchor and Positive Examples closer, and tries to push the Embeddings of Anchor and Negative Examples away from each other.\n",
    "\n",
    "Root mean square difference between Anchor and Positive examples in a batch of N images is:\n",
    "$\n",
    "\\begin{equation}\n",
    "d_p = \\sqrt{\\frac{\\sum_{i=0}^{N-1}(f(a_i) - f(p_i))^2}{N}}\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "Root mean square difference between Anchor and Negative examples in a batch of N images is:\n",
    "$\n",
    "\\begin{equation}\n",
    "d_n = \\sqrt{\\frac{\\sum_{i=0}^{N-1}(f(a_i) - f(n_i))^2}{N}}\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "For each example, we want:\n",
    "$\n",
    "\\begin{equation}\n",
    "d_p \\leq d_n\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "Therefore,\n",
    "$\n",
    "\\begin{equation}\n",
    "d_p - d_n \\leq 0\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "This condition is quite easily satisfied during the training.\n",
    "\n",
    "We will make it non-trivial by adding a margin (alpha):\n",
    "$\n",
    "\\begin{equation}\n",
    "d_p - d_n + \\alpha \\leq 0\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "Given the condition above, the Triplet Loss L is defined as:\n",
    "$\n",
    "\\begin{equation}\n",
    "L = max(d_p - d_n + \\alpha, 0)\n",
    "\\end{equation}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(alpha, emb_dim):\n",
    "    def loss(y_true, y_pred):\n",
    "        anc, pos, neg = y_pred[:, :emb_dim], y_true[:, emb_dim: emb_dim * 2], y_pred[:, 2 * emb_dim:]\n",
    "        dp = tf.reduce_mean(tf.square(anc-pos), axis = 1)\n",
    "        dn = tf.reduce_mean(tf.square(anc-neg), axis = 1)\n",
    "        return tf.maximum(dp - dn + alpha, 0)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 8: Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(batch_size, emb_dim):\n",
    "    while True:\n",
    "        x = create_batch(batch_size)\n",
    "        y = np.zeros((batch_size, 3 * emb_dim))\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 9: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "epochs = 10\n",
    "steps_per_epoch = int(x_train.shape[0] / batch_size)\n",
    "\n",
    "net.compile(loss = triplet_loss(alpha = 0.2, emb_dim=emb_dim), \n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate=0.02))\n",
    "\n",
    "X, Y = x_test[:1000], y_test[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = net.fit(data_generator(batch_size=batch_size, emb_dim=emb_dim), epochs = epochs, steps_per_epoch= steps_per_epoch, verbose = False, callbacks=[PCAPlotter(plt, embedding_model, X, Y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
